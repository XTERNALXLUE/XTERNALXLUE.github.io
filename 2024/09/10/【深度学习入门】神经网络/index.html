<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>【深度学习入门】神经网络 | XTERNALXLUE's blog</title><meta name="author" content="XTERNALXLUE"><meta name="copyright" content="XTERNALXLUE"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="【深度学习入门】神经网络 即便是计算机进行的复杂处理，感知机（理论上）也可以将其表示出来 坏消息是设定权重的工作，即确定合适的、能符合预期的输入与输出的权重，现在还是由人工进行的 神经网络的出现就是为了解决刚才的坏消息 具体地讲，神经网络的一个重要性质是它可以自动地从数据中学习到合适的权重参数 用图来表示神经网络：  最左边的一列称为输入层 最右边的一列称为输出层 中间的一列称为中间层（或隐藏层，">
<meta property="og:type" content="article">
<meta property="og:title" content="【深度学习入门】神经网络">
<meta property="og:url" content="https://xternalxlue.github.io/2024/09/10/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E3%80%91%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="XTERNALXLUE&#39;s blog">
<meta property="og:description" content="【深度学习入门】神经网络 即便是计算机进行的复杂处理，感知机（理论上）也可以将其表示出来 坏消息是设定权重的工作，即确定合适的、能符合预期的输入与输出的权重，现在还是由人工进行的 神经网络的出现就是为了解决刚才的坏消息 具体地讲，神经网络的一个重要性质是它可以自动地从数据中学习到合适的权重参数 用图来表示神经网络：  最左边的一列称为输入层 最右边的一列称为输出层 中间的一列称为中间层（或隐藏层，">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://xternalxlue.github.io/img/bg6.jpg">
<meta property="article:published_time" content="2024-09-10T08:37:06.000Z">
<meta property="article:modified_time" content="2025-01-05T06:19:31.823Z">
<meta property="article:author" content="XTERNALXLUE">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="神经网络">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://xternalxlue.github.io/img/bg6.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://xternalxlue.github.io/2024/09/10/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E3%80%91%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.14.0-b3"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.35/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>(()=>{
      const saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
      
      window.btf = {
        saveToLocal: saveToLocal,
        getScript: (url, attr = {}) => new Promise((resolve, reject) => {
          const script = document.createElement('script')
          script.src = url
          script.async = true
          script.onerror = reject
          script.onload = script.onreadystatechange = function() {
            const loadState = this.readyState
            if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
            script.onload = script.onreadystatechange = null
            resolve()
          }

          Object.keys(attr).forEach(key => {
            script.setAttribute(key, attr[key])
          })

          document.head.appendChild(script)
        }),

        getCSS: (url, id = false) => new Promise((resolve, reject) => {
          const link = document.createElement('link')
          link.rel = 'stylesheet'
          link.href = url
          if (id) link.id = id
          link.onerror = reject
          link.onload = link.onreadystatechange = function() {
            const loadState = this.readyState
            if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
            link.onload = link.onreadystatechange = null
            resolve()
          }
          document.head.appendChild(link)
        }),

        addGlobalFn: (key, fn, name = false, parent = window) => {
          const pjaxEnable = false
          if (!pjaxEnable && key.startsWith('pjax')) return

          const globalFn = parent.globalFn || {}
          const keyObj = globalFn[key] || {}
    
          if (name && keyObj[name]) return
    
          name = name || Object.keys(keyObj).length
          keyObj[name] = fn
          globalFn[key] = keyObj
          parent.globalFn = globalFn
        }
      }
    
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode
      
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(e => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })()</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":true,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '【深度学习入门】神经网络',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-01-05 14:19:31'
}</script><link rel="stylesheet" href="/css/style.css"><meta name="generator" content="Hexo 7.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/./img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">405</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">75</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 文章</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/./img/bg6.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="XTERNALXLUE's blog"><span class="site-name">XTERNALXLUE's blog</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 文章</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">【深度学习入门】神经网络</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-09-10T08:37:06.000Z" title="发表于 2024-09-10 16:37:06">2024-09-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-01-05T06:19:31.823Z" title="更新于 2025-01-05 14:19:31">2025-01-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="【深度学习入门】神经网络"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="【深度学习入门】神经网络"><a href="#【深度学习入门】神经网络" class="headerlink" title="【深度学习入门】神经网络"></a>【深度学习入门】神经网络</h2><blockquote>
<p>即便是计算机进行的复杂处理，感知机（理论上）也可以将其表示出来</p>
<p>坏消息是设定权重的工作，即确定合适的、能符合预期的输入与输出的权重，现在还是由人工进行的</p>
<p>神经网络的出现就是为了解决刚才的坏消息</p>
<p>具体地讲，神经网络的一个重要性质是它可以自动地从数据中学习到合适的权重参数</p>
<p>用图来表示神经网络：</p>
<p><img src="https://img.picgo.net/2024/09/10/sdxx9cd1b96b16e3f17d6.png" alt="sdxx9"></p>
<p>最左边的一列称为输入层</p>
<p>最右边的一列称为输出层</p>
<p>中间的一列称为中间层（或隐藏层，“隐藏”一词的意思是，隐藏层的神经元（和输入层、输出层不同）肉眼看不见）</p>
<hr>
<p><img src="https://img.picgo.net/2024/09/09/sdxx6a8778679b8bf6cd.png" alt="sdxx"></p>
<p>上图的感知机可以用以下数学式表示：</p>
<p><img src="https://img.picgo.net/2024/09/10/sdxx103ed4db91d36fd385.png" alt="sdxx10"></p>
<p>改写后：</p>
<p><img src="https://img.picgo.net/2024/09/10/sdxx117cb1a9389676dfa0.png" alt="sdxx11"></p>
<p><img src="https://img.picgo.net/2024/09/10/sdxx1273cabe5994da7163.png" alt="sdxx12"></p>
<p><em>h</em>（<em>x</em>）函数会将输入信号的总和转换为输出信号，这种函数一般称为激活函数（activation function）</p>
<p>上面的激活函数以阈值为界，一旦输入超过阈值，就切换输出。这样的函数称为“阶跃函数”。</p>
<p>感知机使用了阶跃函数，如果将激活函数从阶跃函数换成其他函数，就可以进入神经网络的世界了</p>
</blockquote>
<hr>
<h3 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h3><p><img src="https://img.picgo.net/2024/09/10/sdxx13d0e315b1ad088d0e.png" alt="sdxx13"></p>
<p>这是神经网络中经常使用的一个激活函数</p>
<p>实际上，感知机和神经网络的主要区别就在于这个激活函数</p>
<hr>
<p>一个简单的阶跃函数Python实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这个只能接受浮点数为参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">step_function</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">if</span> x &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment"># 这个可以接受numpy数组为参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">step_function_numpy</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="comment"># 方式1</span></span><br><span class="line">    <span class="keyword">return</span> np.array(x &gt; <span class="number">0</span>, dtype=<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 方式2</span></span><br><span class="line">    <span class="comment"># y = x &gt; 0</span></span><br><span class="line">    <span class="comment"># return y.astype(np.int)</span></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="comment"># 画图</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_step_function</span>():</span><br><span class="line">    x = np.arange(-<span class="number">5.0</span>, <span class="number">5.0</span>, <span class="number">0.1</span>)</span><br><span class="line">    y = step_function_numpy(x)</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.ylim(-<span class="number">0.1</span>, <span class="number">1.1</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    plot_step_function()</span><br></pre></td></tr></table></figure>

<p>运行结果如下图：</p>
<p><img src="https://img.picgo.net/2024/09/10/sdxx14c67d9adad16a79b3.png" alt="sdxx14"></p>
<hr>
<p>sigmoid函数用Python实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br></pre></td></tr></table></figure>

<p>其图像如下：</p>
<p><img src="https://img.picgo.net/2024/09/10/sdxx1510281cbf0d5b9084.png" alt="sdxx15"></p>
<p>可以看到与阶跃函数的“平滑性”不同</p>
<p>相对于阶跃函数只能返回0或1，sigmoid函数可以返回0.731，0.880等实数</p>
<p>即感知机中神经元之间流动的是0或1的二元信号</p>
<p>而神经网络中流动的是连续的实数值信号</p>
<p>两者均为非线性函数</p>
<hr>
<p>输出值是输入值的常数倍的函数称为线性函数（用数学式表示为<em>h</em>(<em>x</em>) &#x3D; c<em>x</em>。c为常数）</p>
<p>激活函数不能使用线性函数</p>
<p>因为使用线性函数的话，加深神经网络的层数就没有意义了</p>
<p>线性函数的问题在于，不管如何加深层数，总是存在与之等效的“无隐藏层的神经网络”</p>
<p>举例：</p>
<p>把线性函数 <em>h</em>(<em>x</em>) &#x3D; c<em>x</em> 作为激活函数</p>
<p>把<em>y</em>(<em>x</em>) &#x3D; <em>h</em>(<em>h</em>(<em>h</em>(<em>x</em>)))的运算对应3层神经网络</p>
<p>这个运算会进行<em>y</em>(<em>x</em>) &#x3D; c <em>×</em> c <em>×</em> c <em>×</em> <em>x</em>的乘法运算</p>
<p>但可以由<em>y</em>(<em>x</em>) &#x3D; <em>ax</em>（注意，<em>a</em> &#x3D; c^3^ ）这一次乘法运算（即没有隐藏层的神经网络）来表示</p>
<p>无法发挥多层网络带来的优势</p>
<hr>
<h3 id="ReLU函数"><a href="#ReLU函数" class="headerlink" title="ReLU函数"></a>ReLU函数</h3><p>在神经网络发展的历史上，sigmoid函数很早就开始被使用了</p>
<p>而最近则主要使用<strong>ReLU</strong>（Rectified Linear Unit）函数</p>
<p>ReLU函数在输入大于0时，直接输出该值；在输入小于等于0时，输出0</p>
<p>数学表达如下：</p>
<p><img src="https://img.picgo.net/2024/09/10/sdxx16429572ee9f9e4512.png" alt="sdxx16"></p>
<p>用Python实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.maximum(<span class="number">0</span>, x)</span><br></pre></td></tr></table></figure>

<p>函数图像如下：</p>
<p><img src="https://img.picgo.net/2024/09/10/sdxx17f58113d58ee8838a.png" alt="sdxx17"></p>
<hr>
<h3 id="神经网络内积"><a href="#神经网络内积" class="headerlink" title="神经网络内积"></a>神经网络内积</h3><p><img src="https://img.picgo.net/2024/09/10/sdxx18226dd1460b1eb49c.png" alt="sdxx18"></p>
<p>要注意<strong>X</strong>、<strong>W</strong>、<strong>Y</strong>的形状，特别是<strong>X</strong>和<strong>W</strong>的对应维度的元素个数是否一致</p>
<p>使用np.dot（多维数组的点积），可以一次性计算出<strong>Y</strong> 的结果。</p>
<p>这意味着，即便<strong>Y</strong> 的元素个数为100或1000，也可以通过一次运算就计算出结果</p>
<p>如果不使用np.dot，就必须单独计算<strong>Y</strong> 的每一个元素（或者说必须使用for语句），非常麻烦</p>
<hr>
<h3 id="3层神经网络的实现"><a href="#3层神经网络的实现" class="headerlink" title="3层神经网络的实现"></a>3层神经网络的实现</h3><p><img src="https://img.picgo.net/2024/09/10/sdxx23b107a70aaae42750.png" alt="sdxx23"></p>
<p>先了解一下符号：</p>
<p><img src="https://img.picgo.net/2024/09/10/sdxx19d3e7c039b1305618.png" alt="sdxx19"></p>
<p>再看一下从输入层到第1层的第1个神经元的信号传递过程：</p>
<p><img src="https://img.picgo.net/2024/09/10/sdxx207d2fba6c974ff23b.png" alt="sdxx20"></p>
<p>用数学符号表示为：</p>
<p><img src="https://img.picgo.net/2024/09/10/sdxx212eccd7274e1b1f13.png" alt="sdxx21"></p>
<p>用矩阵乘法就可表示为：</p>
<p><img src="https://img.picgo.net/2024/09/10/sdxx224a6041eca491fc46.png" alt="sdxx22"></p>
<p>用Python实现过程如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> activation_function <span class="keyword">import</span> sigmoid, identity_function</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_network</span>():</span><br><span class="line">    network = &#123;&#125;</span><br><span class="line">    network[<span class="string">&#x27;W1&#x27;</span>] = np.array([[<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.5</span>], [<span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.6</span>]])</span><br><span class="line">    network[<span class="string">&#x27;b1&#x27;</span>] = np.array([<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>])</span><br><span class="line">    network[<span class="string">&#x27;W2&#x27;</span>] = np.array([[<span class="number">0.1</span>, <span class="number">0.4</span>], [<span class="number">0.2</span>, <span class="number">0.5</span>], [<span class="number">0.3</span>, <span class="number">0.6</span>]])</span><br><span class="line">    network[<span class="string">&#x27;b2&#x27;</span>] = np.array([<span class="number">0.1</span>, <span class="number">0.2</span>])</span><br><span class="line">    network[<span class="string">&#x27;W3&#x27;</span>] = np.array([[<span class="number">0.1</span>, <span class="number">0.3</span>], [<span class="number">0.2</span>, <span class="number">0.4</span>]])</span><br><span class="line">    network[<span class="string">&#x27;b3&#x27;</span>] = np.array([<span class="number">0.1</span>, <span class="number">0.2</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> network</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">network, x</span>):</span><br><span class="line">    W1, W2, W3 = network[<span class="string">&#x27;W1&#x27;</span>], network[<span class="string">&#x27;W2&#x27;</span>], network[<span class="string">&#x27;W3&#x27;</span>]</span><br><span class="line">    b1, b2, b3 = network[<span class="string">&#x27;b1&#x27;</span>], network[<span class="string">&#x27;b2&#x27;</span>], network[<span class="string">&#x27;b3&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    a1 = np.dot(x, W1) + b1</span><br><span class="line">    z1 = sigmoid(a1)</span><br><span class="line">    a2 = np.dot(z1, W2) + b2</span><br><span class="line">    z2 = sigmoid(a2)</span><br><span class="line">    a3 = np.dot(z2, W3) + b3</span><br><span class="line">    y = identity_function(a3)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">network = init_network()</span><br><span class="line">x = np.array([<span class="number">1.0</span>, <span class="number">0.5</span>])</span><br><span class="line">y = forward(network, x)</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>acvitvation_function.py：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;激活函数模块&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.maximum(<span class="number">0</span>, x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">identity_function</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        用于神经网络的输出</span></span><br><span class="line"><span class="string">        函数的输出是0.0到1.0之间的实数, 且输出值的总和是1, 因此将其输出解释为概率</span></span><br><span class="line"><span class="string">        一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果。</span></span><br><span class="line"><span class="string">        并且，即便使用softmax函数，输出值最大的神经元的位置也不会变</span></span><br><span class="line"><span class="string">        因此，神经网络在进行分类时，输出层的softmax函数可以省略。</span></span><br><span class="line"><span class="string">        在实际的问题中，由于指数函数的运算需要一定的计算机运算量</span></span><br><span class="line"><span class="string">        因此输出层的softmax函数一般会被省略。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    c = np.<span class="built_in">max</span>(x)</span><br><span class="line">    exp_x = np.exp(x - c)  <span class="comment"># 溢出对策</span></span><br><span class="line">    <span class="keyword">return</span> exp_x / np.<span class="built_in">sum</span>(exp_x)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>定义identity_function()函数（也称为“恒等函数”）</p>
<p>并将其作为输出层的激活函数</p>
<p>恒等函数会将输入按原样输出</p>
<p>输出层的激活函数用<em>σ</em>()表示，不同于隐藏层的激活函数<em>h</em>()</p>
<p>&#x3D;&#x3D;一般地，回归问题可以使用恒等函数，二元分类问题可以使用sigmoid函数，多元分类问题可以使用softmax函数&#x3D;&#x3D;</p>
<hr>
<h3 id="softmax函数"><a href="#softmax函数" class="headerlink" title="softmax函数"></a>softmax函数</h3><p><img src="https://img.picgo.net/2024/09/10/sdxx246c115b919e49e728.png" alt="sdxx24"></p>
<p>Python代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">a</span>):</span><br><span class="line">	exp_a = np.exp(a)</span><br><span class="line">    sum_exp_a = np.<span class="built_in">sum</span>(exp_a)</span><br><span class="line">    y = exp_a / sum_exp_a</span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>

<p>用图表示如下：</p>
<p><img src="https://img.picgo.net/2024/09/10/sdxx25893bb2c663de7d0d.png" alt="sdxx25"></p>
<p>其Python实现在上面activation_function.py中有</p>
<p>softmax函数在运算上有一定缺陷（容易溢出</p>
<p>可通过如下方式改进：</p>
<p><img src="https://img.picgo.net/2024/09/10/sdxx261ecff12e16c3970f.png" alt="sdxx26"></p>
<p>改进后Python代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">a</span>):</span><br><span class="line">	c = np.<span class="built_in">max</span>(a)</span><br><span class="line">    exp_a = np.exp(a - c) <span class="comment"># 溢出对策</span></span><br><span class="line">    sum_exp_a = np.<span class="built_in">sum</span>(exp_a)</span><br><span class="line">    y = exp_a / sum_exp_a</span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>

<p>softmax函数的输出是0.0到1.0之间的实数</p>
<p>并且输出值的总和是1，这是softmax函数的一个重要性质</p>
<p>正因此，可以把softmax函数的输出解释为“概率”</p>
<p>即便使用了softmax函数，各个元素之间的大小关系也不会改变</p>
<p>这是因为指数函数（<em>y</em> &#x3D; exp(<em>x</em>)）是单调递增函数</p>
<p>一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果</p>
<p>并且，即便使用softmax函数，输出值最大的神经元的位置也不会变</p>
<p>因此，神经网络在进行分类时，输出层的softmax函数可以省略</p>
<p>在实际的问题中，由于指数函数的运算需要一定的计算机运算量，因此输出层的softmax函数一般会被省略</p>
<hr>
<p>输出层的神经元数量需要根据待解决的问题来决定</p>
<p>接下来尝试使用已经训练好的神经网络来解决手写数字识别问题（分类问题）</p>
<p>即假设学习已经全部结束，我们使用学习到的参数，先实现神经网络的“推理处理”</p>
<p>这个推理处理也称为神经网络的前向传播（forward propagation）</p>
<hr>
<h3 id="使用的数据集是MNIST手写数字图像集"><a href="#使用的数据集是MNIST手写数字图像集" class="headerlink" title="使用的数据集是MNIST手写数字图像集"></a>使用的数据集是MNIST手写数字图像集</h3><blockquote>
<p>这里只涉及神经网络的使用，而不涉及神经网络的构建与训练</p>
</blockquote>
<p><img src="https://img.picgo.net/2024/09/10/sdxx277d5ef9f74bb56233.png" alt="sdxx27"></p>
<p>MNIST的图像数据是28 × 28像素的灰度图像（1通道），各个像素的取值在0到255之间</p>
<p>流程如下：</p>
<p>运行随书代码mnist.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">except</span> ImportError:</span><br><span class="line">    <span class="keyword">raise</span> ImportError(<span class="string">&#x27;You should use Python 3.x&#x27;</span>)</span><br><span class="line"><span class="keyword">import</span> os.path</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    MNIST数据集下载及处理代码</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">url_base = <span class="string">&#x27;http://yann.lecun.com/exdb/mnist/&#x27;</span></span><br><span class="line">key_file = &#123;</span><br><span class="line">    <span class="string">&#x27;train_img&#x27;</span>: <span class="string">&#x27;train-images-idx3-ubyte.gz&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;train_label&#x27;</span>: <span class="string">&#x27;train-labels-idx1-ubyte.gz&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;test_img&#x27;</span>: <span class="string">&#x27;t10k-images-idx3-ubyte.gz&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;test_label&#x27;</span>: <span class="string">&#x27;t10k-labels-idx1-ubyte.gz&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">dataset_dir = os.path.dirname(os.path.abspath(__file__))</span><br><span class="line">save_file = dataset_dir + <span class="string">&quot;/mnist.pkl&quot;</span></span><br><span class="line"></span><br><span class="line">train_num = <span class="number">60000</span></span><br><span class="line">test_num = <span class="number">10000</span></span><br><span class="line">img_dim = (<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">img_size = <span class="number">784</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_download</span>(<span class="params">file_name</span>):</span><br><span class="line">    file_path = dataset_dir + <span class="string">&quot;/&quot;</span> + file_name</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(file_path):</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Downloading &quot;</span> + file_name + <span class="string">&quot; ... &quot;</span>)</span><br><span class="line">    urllib.request.urlretrieve(url_base + file_name, file_path)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Done&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download_mnist</span>():</span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> key_file.values():</span><br><span class="line">        _download(v)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_load_label</span>(<span class="params">file_name</span>):</span><br><span class="line">    file_path = dataset_dir + <span class="string">&quot;/&quot;</span> + file_name</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Converting &quot;</span> + file_name + <span class="string">&quot; to NumPy Array ...&quot;</span>)</span><br><span class="line">    <span class="keyword">with</span> gzip.<span class="built_in">open</span>(file_path, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        labels = np.frombuffer(f.read(), np.uint8, offset=<span class="number">8</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Done&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> labels</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_load_img</span>(<span class="params">file_name</span>):</span><br><span class="line">    file_path = dataset_dir + <span class="string">&quot;/&quot;</span> + file_name</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Converting &quot;</span> + file_name + <span class="string">&quot; to NumPy Array ...&quot;</span>)</span><br><span class="line">    <span class="keyword">with</span> gzip.<span class="built_in">open</span>(file_path, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        data = np.frombuffer(f.read(), np.uint8, offset=<span class="number">16</span>)</span><br><span class="line">    data = data.reshape(-<span class="number">1</span>, img_size)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Done&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_convert_numpy</span>():</span><br><span class="line">    dataset = &#123;&#125;</span><br><span class="line">    dataset[<span class="string">&#x27;train_img&#x27;</span>] = _load_img(key_file[<span class="string">&#x27;train_img&#x27;</span>])</span><br><span class="line">    dataset[<span class="string">&#x27;train_label&#x27;</span>] = _load_label(key_file[<span class="string">&#x27;train_label&#x27;</span>])</span><br><span class="line">    dataset[<span class="string">&#x27;test_img&#x27;</span>] = _load_img(key_file[<span class="string">&#x27;test_img&#x27;</span>])</span><br><span class="line">    dataset[<span class="string">&#x27;test_label&#x27;</span>] = _load_label(key_file[<span class="string">&#x27;test_label&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_mnist</span>():</span><br><span class="line">    download_mnist()</span><br><span class="line">    dataset = _convert_numpy()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Creating pickle file ...&quot;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(save_file, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        pickle.dump(dataset, f, -<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Done!&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_change_one_hot_label</span>(<span class="params">X</span>):</span><br><span class="line">    T = np.zeros((X.size, <span class="number">10</span>))</span><br><span class="line">    <span class="keyword">for</span> idx, row <span class="keyword">in</span> <span class="built_in">enumerate</span>(T):</span><br><span class="line">        row[X[idx]] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> T</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_mnist</span>(<span class="params">normalize=<span class="literal">True</span>, flatten=<span class="literal">True</span>, one_hot_label=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;读入MNIST数据集</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    normalize : 将图像的像素值正规化为0.0~1.0</span></span><br><span class="line"><span class="string">    one_hot_label : </span></span><br><span class="line"><span class="string">        one_hot_label为True的情况下，标签作为one-hot数组返回</span></span><br><span class="line"><span class="string">        one-hot数组是指[0,0,1,0,0,0,0,0,0,0]这样的数组</span></span><br><span class="line"><span class="string">    flatten : 是否将图像展开为一维数组</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    (训练图像, 训练标签), (测试图像, 测试标签)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(save_file):</span><br><span class="line">        init_mnist()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(save_file, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        dataset = pickle.load(f)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> normalize:</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> (<span class="string">&#x27;train_img&#x27;</span>, <span class="string">&#x27;test_img&#x27;</span>):</span><br><span class="line">            dataset[key] = dataset[key].astype(np.float32)</span><br><span class="line">            dataset[key] /= <span class="number">255.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> one_hot_label:</span><br><span class="line">        dataset[<span class="string">&#x27;train_label&#x27;</span>] = _change_one_hot_label(dataset[<span class="string">&#x27;train_label&#x27;</span>])</span><br><span class="line">        dataset[<span class="string">&#x27;test_label&#x27;</span>] = _change_one_hot_label(dataset[<span class="string">&#x27;test_label&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> flatten:</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> (<span class="string">&#x27;train_img&#x27;</span>, <span class="string">&#x27;test_img&#x27;</span>):</span><br><span class="line">            dataset[key] = dataset[key].reshape(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (dataset[<span class="string">&#x27;train_img&#x27;</span>], dataset[<span class="string">&#x27;train_label&#x27;</span>]), (dataset[<span class="string">&#x27;test_img&#x27;</span>], dataset[<span class="string">&#x27;test_label&#x27;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    init_mnist()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>下载这些数据集</p>
<p>t10k-images-idx3-ubyte.gz</p>
<p>t10k-labels-idx1-ubyte.gz</p>
<p>train-images-idx3-ubyte.gz</p>
<p>train-labels-idx1-ubyte.gz</p>
<p>（但实际上好像脚本提供的入口失效了，这些数据集另外找的，，</p>
<p>下载好后放到dataset软件包里（与mnist.py是一个软件包里，没有就自建软件包</p>
<p>然后新建py文件（不在dataset软件包</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os, sys</span><br><span class="line"></span><br><span class="line">sys.path.append(os.pardir)</span><br><span class="line"><span class="keyword">from</span> dataset.mnist <span class="keyword">import</span> load_mnist</span><br><span class="line"></span><br><span class="line">(x_train, t_train), (x_test, t_test) = load_mnist(flatten=<span class="literal">True</span>, normalize=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(x_train.shape)</span><br><span class="line"><span class="built_in">print</span>(t_train.shape)</span><br><span class="line"><span class="built_in">print</span>(x_test.shape)</span><br><span class="line"><span class="built_in">print</span>(t_test.shape)</span><br></pre></td></tr></table></figure>

<p>运行就可在终端中看到数据信息如下：</p>
<p><img src="https://img.picgo.net/2024/09/10/sdxx289f263854fa0e6000.png" alt="sdxx28"></p>
<p>(第一次可能有点慢，后来在dataset软件包里数据集会被打包成mnisk.pkl，就会更快)</p>
<p>然后运行mnist_show.py可以查看一张数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"><span class="keyword">import</span> sys, os</span><br><span class="line"></span><br><span class="line">sys.path.append(os.pardir)  <span class="comment"># 为了导入父目录的文件而进行的设定</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> dataset.mnist <span class="keyword">import</span> load_mnist</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">img_show</span>(<span class="params">img</span>):</span><br><span class="line">    pil_img = Image.fromarray(np.uint8(img))</span><br><span class="line">    pil_img.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取MNIST数据集</span></span><br><span class="line">(x_train, t_train), (x_test, t_test) = load_mnist(flatten=<span class="literal">True</span>, normalize=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一个图像数据</span></span><br><span class="line">img = x_train[<span class="number">0</span>]</span><br><span class="line">label = t_train[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(label)  <span class="comment"># 5</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(img.shape)  <span class="comment"># (784,)</span></span><br><span class="line">img = img.reshape(<span class="number">28</span>, <span class="number">28</span>)  <span class="comment"># 把图像的形状变为原来的尺寸</span></span><br><span class="line"><span class="built_in">print</span>(img.shape)  <span class="comment"># (28, 28)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示图像</span></span><br><span class="line">img_show(img)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>运行后即可看到第一张数据手写“5”的图片以及终端打印的图片信息</p>
<hr>
<p>neuralnet_mnist.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"><span class="keyword">import</span> sys, os</span><br><span class="line"></span><br><span class="line">sys.path.append(os.pardir)  <span class="comment"># 为了导入父目录的文件而进行的设定</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">from</span> dataset.mnist <span class="keyword">import</span> load_mnist</span><br><span class="line"><span class="keyword">from</span> common.functions <span class="keyword">import</span> sigmoid, softmax</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_data</span>():</span><br><span class="line">    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class="literal">True</span>, flatten=<span class="literal">True</span>, one_hot_label=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> x_test, t_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_network</span>():</span><br><span class="line">    <span class="comment"># 加载已训练好的权重模型</span></span><br><span class="line">    <span class="comment"># sample_weight.pkl由随书文件提供，相当于省略了训练过程，直接跑结果</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;sample_weight.pkl&quot;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:	</span><br><span class="line">        network = pickle.load(f)</span><br><span class="line">    <span class="keyword">return</span> network</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">network, x</span>):</span><br><span class="line">    W1, W2, W3 = network[<span class="string">&#x27;W1&#x27;</span>], network[<span class="string">&#x27;W2&#x27;</span>], network[<span class="string">&#x27;W3&#x27;</span>]</span><br><span class="line">    b1, b2, b3 = network[<span class="string">&#x27;b1&#x27;</span>], network[<span class="string">&#x27;b2&#x27;</span>], network[<span class="string">&#x27;b3&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    a1 = np.dot(x, W1) + b1</span><br><span class="line">    z1 = sigmoid(a1)</span><br><span class="line">    a2 = np.dot(z1, W2) + b2</span><br><span class="line">    z2 = sigmoid(a2)</span><br><span class="line">    a3 = np.dot(z2, W3) + b3</span><br><span class="line">    y = softmax(a3)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x, t = get_data()</span><br><span class="line">network = init_network()</span><br><span class="line">accuracy_cnt = <span class="number">0</span>	<span class="comment"># 统计正确的次数</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x)):</span><br><span class="line">    y = predict(network, x[i])</span><br><span class="line">    p = np.argmax(y)  <span class="comment"># 获取概率最高的元素的索引</span></span><br><span class="line">    <span class="keyword">if</span> p == t[i]:</span><br><span class="line">        accuracy_cnt += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy:&quot;</span> + <span class="built_in">str</span>(<span class="built_in">float</span>(accuracy_cnt) / <span class="built_in">len</span>(x)))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>运行后终端输出<code>Accuracy:0.9352</code></p>
<p>此时只是学习如何使用神经网络，而不涉及构建、训练</p>
<p>上述代码虽然运行蛮快的，但其实是用for循环逐张处理</p>
<p>可以修改代码，一次批量处理以提高效率，这种打包式的输入数据称为批（batch）</p>
<p>修改后的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"><span class="keyword">import</span> sys, os</span><br><span class="line"></span><br><span class="line">sys.path.append(os.pardir)  <span class="comment"># 为了导入父目录的文件而进行的设定</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">from</span> dataset.mnist <span class="keyword">import</span> load_mnist</span><br><span class="line"><span class="keyword">from</span> common.functions <span class="keyword">import</span> sigmoid, softmax</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_data</span>():</span><br><span class="line">    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class="literal">True</span>, flatten=<span class="literal">True</span>, one_hot_label=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> x_test, t_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_network</span>():</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;sample_weight.pkl&quot;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        network = pickle.load(f)</span><br><span class="line">    <span class="keyword">return</span> network</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">network, x</span>):</span><br><span class="line">    w1, w2, w3 = network[<span class="string">&#x27;W1&#x27;</span>], network[<span class="string">&#x27;W2&#x27;</span>], network[<span class="string">&#x27;W3&#x27;</span>]</span><br><span class="line">    b1, b2, b3 = network[<span class="string">&#x27;b1&#x27;</span>], network[<span class="string">&#x27;b2&#x27;</span>], network[<span class="string">&#x27;b3&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    a1 = np.dot(x, w1) + b1</span><br><span class="line">    z1 = sigmoid(a1)</span><br><span class="line">    a2 = np.dot(z1, w2) + b2</span><br><span class="line">    z2 = sigmoid(a2)</span><br><span class="line">    a3 = np.dot(z2, w3) + b3</span><br><span class="line">    y = softmax(a3)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x, t = get_data()</span><br><span class="line">network = init_network()</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---做了更改的地方在下方---</span></span><br><span class="line">batch_size = <span class="number">100</span>  <span class="comment"># 批数量</span></span><br><span class="line">accuracy_cnt = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(x), batch_size):</span><br><span class="line">    x_batch = x[i:i + batch_size]</span><br><span class="line">    y_batch = predict(network, x_batch)</span><br><span class="line">    p = np.argmax(y_batch, axis=<span class="number">1</span>)</span><br><span class="line">    accuracy_cnt += np.<span class="built_in">sum</span>(p == t[i:i + batch_size])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy:&quot;</span> + <span class="built_in">str</span>(<span class="built_in">float</span>(accuracy_cnt) / <span class="built_in">len</span>(x)))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>运行结果和上面的是一样的</p>
<hr>
<p>接下来谈谈对神经网络的学习</p>
<p>所谓“从数据中学习”，是指可以由数据自动决定权重参数的值</p>
<p>如果所有的参数都需要人工决定的话，工作量就太大了</p>
<p>在实际的神经网络中，参数的数量成千上万</p>
<p>在层数更深的深度学习中，参数的数量甚至可以上亿</p>
<p>想要人工决定这些参数的值是不可能的</p>
<p>数据是机器学习的命根子，是机器学习的核心</p>
<p>是由数据驱动的</p>
<blockquote>
<p>通常要解决某个问题，特别是需要发现某种模式时，人们一般会综合考虑各种因素后再给出回答。</p>
<p>“这个问题好像有这样的规律性？”“不对，可能原因在别的地方。”</p>
<p>——类似这样，人们以自己的经验和直觉为线索，通过反复试验推进工作</p>
<p>而机器学习的方法则极力避免人为介入，尝试从收集到的数据中发现答案（模式）</p>
<p>神经网络或深度学习则比以往的机器学习方法更能避免人为介入</p>
</blockquote>
<hr>
<p>思考：如何对一张图片上的数字进行分类？</p>
<p>人看到数字能很轻易的说出是几</p>
<p>但却很难说出是基于何种规律</p>
<p>而且每个人都有不同的书写习惯</p>
<p>人为的发现其中存在的规律是困难的</p>
<p>另外一种方案是</p>
<p>先从图像中提取特征量 </p>
<p>再用机器学习技术学习这些特征量的模式</p>
<p>这里所说的“特征量”是指可以从输入数据（输入图像）中准确地提取本质数据（重要的数据）的转换器</p>
<p>图像的特征量通常表示为向量的形式</p>
<p>在计算机视觉领域常用的特征量包括SIFT、SURF和HOG等</p>
<p>使用这些特征量将图像数据转换为向量，然后对转换后的向量使用机器学习中的SVM、KNN等分类器进行学习</p>
<p>与从零开始想出算法相比，这种方法可以更高效地解决问题，也能减轻人的负担</p>
<p>对于不同的问题，必须使用合适的特征量（必须设计专门的特征量），才能得到好的结果</p>
<p>还有一种方法</p>
<p>神经网络直接学习图像本身</p>
<p>用图表示这三种方式如下：</p>
<p><img src="https://img.picgo.net/2024/09/10/sdxx292d2fabcb1557e103.png" alt="sdxx29"></p>
<blockquote>
<p>深度学习有时也称为端到端机器学习（end-to-end machine learning）</p>
<p>这里所说的端到端是指从一端到另一端的意思</p>
<p>也就是从原始数据（输入）中获得目标结果（输出）的意思</p>
</blockquote>
<hr>
<p>机器学习中，一般将数据分为训练数据（或监督数据）和测试数据两部分来进行学习和实验等</p>
<p>将数据分为训练数据和测试数据，是因为我们追求的是模型的泛化能力</p>
<p>为了正确评价模型的泛化能力，就必须划分训练数据和测试数据</p>
<p>泛化能力是指处理未被观察过的数据（不包含在训练数据中的数据）的能力</p>
<p>获得泛化能力是机器学习的最终目标</p>
<p>仅仅用一个数据集去学习和评价参数，是无法进行正确评价的</p>
<p>这样会导致可以顺利地处理某个数据集，但无法处理其他数据集的情况</p>
<p>只对某个数据集过度拟合的状态称为过拟合（over fitting）</p>
<hr>
<p>神经网络以某个指标为线索寻找最优权重参数</p>
<p>神经网络的学习中所用的指标称为损失函数（loss function）</p>
<p>这个损失函数可以使用任意函数</p>
<p>但一般用均方误差和交叉熵误差等</p>
<hr>
<h3 id="均方误差"><a href="#均方误差" class="headerlink" title="均方误差"></a>均方误差</h3><p>可以用作损失函数的函数有很多，其中最有名的是均方误差（mean squared error）</p>
<p><img src="https://img.picgo.net/2024/09/10/sdxx30c8cc2865a26fb5b5.png" alt="sdxx30"></p>
<p>y<del>k</del>是表示神经网络的输出，t<del>k</del>表示监督数据，<em>k</em>表示数据的维数</p>
<p>Python实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mean_squared_error</span>(<span class="params">y, t</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * np.<span class="built_in">sum</span>((y - t) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">t = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">y = [<span class="number">0.1</span>, <span class="number">0.05</span>, <span class="number">0.6</span>, <span class="number">0.0</span>, <span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">0.0</span>, <span class="number">0.1</span>, <span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line"><span class="built_in">print</span>(mean_squared_error(np.array(y), np.array(t)))</span><br><span class="line"><span class="comment"># &quot;2的概率最高, 输出为0.09750000000000003, 表示损失函数很小&quot;</span></span><br><span class="line"></span><br><span class="line">y = [<span class="number">0.1</span>, <span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">0.0</span>, <span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">0.0</span>, <span class="number">0.6</span>, <span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line"><span class="built_in">print</span>(mean_squared_error(np.array(y), np.array(t)))</span><br><span class="line"><span class="comment"># &quot;6的概率最高, 但实际上是2, 输出为0.5975000000000001, 误差较大&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<hr>
<h3 id="one-hot表示"><a href="#one-hot表示" class="headerlink" title="one-hot表示"></a>one-hot表示</h3><p>将正确解标签表示为1，其他标签表示为0的表示方法称为<strong>one-hot</strong>表示</p>
<hr>
<h3 id="交叉熵误差"><a href="#交叉熵误差" class="headerlink" title="交叉熵误差"></a>交叉熵误差</h3><p>除了均方误差之外，交叉熵误差（cross entropy error）也经常被用作损失函数</p>
<p>交叉熵误差如下式所示：</p>
<p><img src="https://img.picgo.net/2024/09/10/sdxx310ea9cc1a59487799.png" alt="sdxx31"></p>
<p>y<del>k</del>是神经网络的输出，t<del>k</del>是正确解标签</p>
<p>只计算对应正确解标签的输出的自然对数</p>
<p>即交叉熵误差的值是由正确解标签所对应的输出结果决定的</p>
<p>用Python实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy_error</span>(<span class="params">y, t</span>):</span><br><span class="line">    delta = <span class="number">1e-7</span>  <span class="comment"># avoid log(0)</span></span><br><span class="line">    <span class="keyword">return</span> -np.<span class="built_in">sum</span>(t * np.log(y + delta))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">t = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">y = [<span class="number">0.1</span>, <span class="number">0.05</span>, <span class="number">0.6</span>, <span class="number">0.0</span>, <span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">0.0</span>, <span class="number">0.1</span>, <span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line"><span class="built_in">print</span>(cross_entropy_error(np.array(y), np.array(t)))</span><br><span class="line"><span class="comment"># 0.510825457099338</span></span><br><span class="line"></span><br><span class="line">y = [<span class="number">0.1</span>, <span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">0.0</span>, <span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">0.0</span>, <span class="number">0.6</span>, <span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line"><span class="built_in">print</span>(cross_entropy_error(np.array(y), np.array(t)))</span><br><span class="line"><span class="comment"># 2.302584092994546</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<hr>
<p>机器学习使用训练数据进行学习</p>
<p>严格来说，就是针对训练数据计算损失函数的值，找出使该值尽可能小的参数</p>
<p>因此，计算损失函数时必须将所有的训练数据作为对象</p>
<p>以交叉熵误差为例，可以写成下面的式子：</p>
<p><img src="https://img.picgo.net/2024/09/11/sdxx32f83bc171ddc1ee2f.png" alt="sdxx32"></p>
<p>假设数据有<em>N</em>个</p>
<p>t<del>n~~k</del>表示第<em>n</em>个数据的第<em>k</em>个元素的值</p>
<p>（y<del>n~~k</del>是神经网络的输出，t<del>n~~k</del>是监督数据）</p>
<p>把求单个数据的损失函数的式扩大到了<em>N</em>份数据，不过最后还要除以N进行正规化</p>
<p>从全部数据中选出一部分，作为全部数据的“近似”</p>
<p>神经网络的学习也是从训练数据中随机选出一批数据（称为mini-batch, 小批量），然后对每个mini-batch进行学习</p>
<p>学习方式称为<strong>mini-batch</strong>学习</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys, os</span><br><span class="line"></span><br><span class="line">sys.path.append(os.pardir)</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> dataset.mnist <span class="keyword">import</span> load_mnist</span><br><span class="line"></span><br><span class="line">(x_train, t_train), (x_test, t_test) = \</span><br><span class="line">    load_mnist(normalize=<span class="literal">True</span>, one_hot_label=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x_train.shape)  <span class="comment"># (60000, 784)</span></span><br><span class="line"><span class="built_in">print</span>(t_train.shape)  <span class="comment"># (60000, 10)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从这个训练数据中随机抽取10笔数据</span></span><br><span class="line">train_size = x_train.shape[<span class="number">0</span>]</span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">batch_mask = np.random.choice(train_size, batch_size)</span><br><span class="line">x_batch = x_train[batch_mask]</span><br><span class="line">t_batch = t_train[batch_mask]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实现对应mini-batch的交叉熵误差</span></span><br><span class="line"><span class="comment"># 可以同时处理单个数据和批量数据（数据作为batch集中输入）两种情况的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy_error</span>(<span class="params">y, t</span>):</span><br><span class="line">    <span class="keyword">if</span> y.ndim == <span class="number">1</span>:</span><br><span class="line">        t = t.reshape(<span class="number">1</span>, t.size)</span><br><span class="line">        y = y.reshape(<span class="number">1</span>, y.size)</span><br><span class="line"></span><br><span class="line">    batch_size = y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> -np.<span class="built_in">sum</span>(t * np.log(y)) / batch_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 当监督数据是标签形式（非one-hot表示，而是像“2”“7”这样的标签）时，交叉熵误差可通过如下代码实现</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy_error2</span>(<span class="params">y, t</span>):</span><br><span class="line">    <span class="keyword">if</span> y.ndim == <span class="number">1</span>:</span><br><span class="line">        t = t.reshape(<span class="number">1</span>, t.size)</span><br><span class="line">        y = y.reshape(<span class="number">1</span>, y.size)</span><br><span class="line"></span><br><span class="line">    batch_size = y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> -np.<span class="built_in">sum</span>(np.log(y[np.arange(batch_size), t] + <span class="number">1e-7</span>)) / batch_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">	解释一下np.log( y[np.arange(batch_size), t] )</span></span><br><span class="line"><span class="string">	np.arange (batch_size)会生成一个从0到batch_size-1的数组</span></span><br><span class="line"><span class="string">	比如当batch_size为5时，np.arange(batch_size)会生成一个NumPy 数组[0, 1, 2, 3, 4]</span></span><br><span class="line"><span class="string">	因为t中标签是以[2, 7, 0, 9, 4]的形式存储的</span></span><br><span class="line"><span class="string">	所以y[np.arange(batch_size),t]能抽出各个数据的正确解标签对应的神经网络的输出</span></span><br><span class="line"><span class="string">	（在这个例子中，y[np.arange(batch_size), t]会生成NumPy数组[y[0,2], y[1,7], y[2,0], y[3,9], y[4,4]]）</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>在神经网络的学习中，寻找最优参数（权重和偏置）时，</p>
<p>要寻找使损失函数的值尽可能小的参数。</p>
<p>为了找到使损失函数的值尽可能小的地方，</p>
<p>需要计算参数的导数（确切地讲是梯度），然后以这个导数为指引，逐步更新参数的值</p>
<p>假设有一个神经网络</p>
<p>现在关注这个神经网络中的某一个权重参数</p>
<p>此时对该权重参数的损失函数求导</p>
<p>表示的是“如果稍微改变这个权重参数的值，损失函数的值会如何变化”</p>
<ul>
<li><p>如果导数的值为负通过使该权重参数向正方向改变，可以减小损失函数的值</p>
</li>
<li><p>反之，如果导数的值为正，则通过使该权重参数向负方向改变，可以减小损失函数的值</p>
</li>
<li><p>不过，当导数的值为0时，无论权重参数向哪个方向变化，损失函数的值都不会改变，此时该权重参数的更新会停在此处</p>
</li>
</ul>
<p>之所以不能用识别精度作为指标</p>
<p>是因为这样一来绝大多数地方的导数都会变为0</p>
<p>导致参数无法更新</p>
<hr>
<p>识别精度对微小的参数变化基本上没有什么反应</p>
<p>即便有反应，它的值也是不连续地、突然地变化</p>
<p>作为激活函数的阶跃函数也有同样的情况</p>
<p>出于相同的原因，如果使用阶跃函数作为激活函数，神经网络的学习将无法进行</p>
<hr>
<p>Python实现求导</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不好的实现示例，存在舍入误差</span></span><br><span class="line"><span class="comment"># 指因省略小数的精细部分的数值（比如，小数点第8位以后的数值）而造成最终的计算结果上的误差</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">numerical_diff</span>(<span class="params">f, x</span>):</span><br><span class="line">	h = <span class="number">10e-50</span></span><br><span class="line">	<span class="keyword">return</span> (f(x+h) - f(x)) / h</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了减小这个误差，我们可以计算函数f在(x + h)和(x − h)之间的差分</span></span><br><span class="line"><span class="comment"># 因为这种计算方法以x为中心，计算它左右两边的差分，所以也称为中心差分</span></span><br><span class="line"><span class="comment"># 而(x + h)和x之间的差分称为前向差分</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">numerical_diff</span>(<span class="params">f, x</span>):</span><br><span class="line">	h = <span class="number">1e-4</span> <span class="comment"># 0.0001</span></span><br><span class="line">    <span class="keyword">return</span> (f(x+h) - f(x-h)) / (<span class="number">2</span>*h)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<hr>
<p>由全部变量的偏导数汇总而成的向量称为梯度（gradient）</p>
<p>用Python实现梯度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">numerical_gradient</span>(<span class="params">f, x</span>):</span><br><span class="line">     h = <span class="number">1e-4</span></span><br><span class="line">     grad = np.zeros_like(x) <span class="comment"># 生成和x形状相同的0数组</span></span><br><span class="line">     <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(x.size):</span><br><span class="line">		tmp_val = x[idx]</span><br><span class="line">         <span class="comment"># f(x+h)的计算</span></span><br><span class="line">         x[idx] = tmp_val + h</span><br><span class="line">         fxh1 = f(x)</span><br><span class="line">            </span><br><span class="line">         <span class="comment"># f(x-h)的计算</span></span><br><span class="line">         x[idx] = tmp_val - h</span><br><span class="line">         fxh2 = f(x)</span><br><span class="line">        </span><br><span class="line">         grad[idx] = (fxh1 - fxh2) / (<span class="number">2</span>*h)</span><br><span class="line">         x[idx] = tmp_val <span class="comment"># 还原值</span></span><br><span class="line">        </span><br><span class="line">     <span class="keyword">return</span> grad</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Python实现梯度下降法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">f, init_x, lr=<span class="number">0.01</span>, step_num=<span class="number">100</span></span>):		<span class="comment"># lr是指learning rate</span></span><br><span class="line">     x = init_x</span><br><span class="line">     <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(step_num):</span><br><span class="line">         grad = numerical_gradient(f, x)	<span class="comment"># 计算梯度</span></span><br><span class="line">         x -= lr * grad	<span class="comment"># 更新参数</span></span><br><span class="line">        </span><br><span class="line">     <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 用梯度法求f(x0 + x1) = x0^2 + x1^2的最小值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">function</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x[<span class="number">0</span>]**<span class="number">2</span> + x[<span class="number">1</span>]**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">init_x = np.array([-<span class="number">3.0</span>, <span class="number">4.0</span>])</span><br><span class="line">gradient_descent(function, init_x=init_x, lr=<span class="number">0.1</span>, step_num=<span class="number">100</span>)</span><br><span class="line"><span class="comment"># array([ -6.11110793e-10, 8.14814391e-10])</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="https://img.picgo.net/2024/09/11/sdxx33fcb01ef15bb43d3f.png" alt="sdxx33"></p>
<p>学习率过大或者过小都无法得到好的结果</p>
<p>这样的参数称为超参数</p>
<p>这是一种和神经网络的参数（权重和偏置）性质不同的参数。</p>
<p>相对于神经网络的权重参数是通过训练数据和学习算法自动获得的</p>
<p>学习率这样的超参数则是人工设定的。</p>
<p>一般来说，超参数需要尝试多个值，以便找到一种可以使学习顺利进行的设定</p>
<hr>
<p>以一个简单的神经网络为例，来实现求梯度的代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys, os</span><br><span class="line"></span><br><span class="line">sys.path.append(os.pardir)  <span class="comment"># 为了导入父目录中的文件而进行的设定</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> common.functions <span class="keyword">import</span> softmax, cross_entropy_error</span><br><span class="line"><span class="keyword">from</span> common.gradient <span class="keyword">import</span> numerical_gradient</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">simpleNet</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.W = np.random.randn(<span class="number">2</span>, <span class="number">3</span>)  <span class="comment"># 用高斯分布进行初始化</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, x</span>):  <span class="comment"># 前向传播</span></span><br><span class="line">        <span class="keyword">return</span> np.dot(x, self.W)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">self, x, t</span>):  <span class="comment"># 计算损失函数</span></span><br><span class="line">        z = self.predict(x)</span><br><span class="line">        y = softmax(z)</span><br><span class="line">        loss = cross_entropy_error(y, t)    <span class="comment"># 计算交叉熵损失</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">0.6</span>, <span class="number">0.9</span>])</span><br><span class="line">t = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">net = simpleNet()</span><br><span class="line"></span><br><span class="line">f = <span class="keyword">lambda</span> w: net.loss(x, t)</span><br><span class="line">dW = numerical_gradient(f, net.W)    <span class="comment"># 计算W的梯度</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(dW)  <span class="comment"># 输出梯度值, 确认更新方向</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<hr>
<p>“损失函数”“mini-batch”“梯度”“梯度下降法”等已经介绍完毕</p>
<p>接下来确认神经网络的学习步骤</p>
<h4 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h4><p>神经网络存在合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为“学习”。</p>
<p>神经网络的学习分成下面4个步骤：</p>
<ol>
<li><p><strong>mini-batch</strong>：从训练数据中随机选出一部分数据，这部分数据称为mini-batch。我们的目标是减小mini-batch的损失函数的值</p>
</li>
<li><p>计算梯度：为了减小mini-batch的损失函数的值，需要求出各个权重参数的梯度</p>
<p>梯度表示损失函数的值减小最多的方向</p>
</li>
<li><p>更新参数：将权重参数沿梯度方向进行微小更新</p>
</li>
<li><p>重复：重复步骤1、步骤2、步骤3</p>
</li>
</ol>
<p>这个方法通过梯度下降法更新参数</p>
<p>不过因为这里使用的数据是随机选择的mini batch数据</p>
<p>所以又称为随机梯度下降法（stochastic gradient descent）</p>
<p>深度学习的很多框架中，随机梯度下降法一般由一个名为<strong>SGD</strong>的函数来实现</p>
<hr>
<h3 id="2层神经网络的类"><a href="#2层神经网络的类" class="headerlink" title="2层神经网络的类"></a>2层神经网络的类</h3><p>two_layer_net.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys, os</span><br><span class="line">sys.path.append(os.pardir)  <span class="comment"># 为了导入父目录的文件而进行的设定</span></span><br><span class="line"><span class="keyword">from</span> common.functions <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> common.gradient <span class="keyword">import</span> numerical_gradient</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TwoLayerNet</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size, weight_init_std=<span class="number">0.01</span></span>):</span><br><span class="line">        <span class="comment"># 初始化权重</span></span><br><span class="line">        self.params = &#123;&#125;</span><br><span class="line">        self.params[<span class="string">&#x27;W1&#x27;</span>] = weight_init_std * np.random.randn(input_size, hidden_size)</span><br><span class="line">        self.params[<span class="string">&#x27;b1&#x27;</span>] = np.zeros(hidden_size)</span><br><span class="line">        self.params[<span class="string">&#x27;W2&#x27;</span>] = weight_init_std * np.random.randn(hidden_size, output_size)</span><br><span class="line">        self.params[<span class="string">&#x27;b2&#x27;</span>] = np.zeros(output_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, x</span>):</span><br><span class="line">        W1, W2 = self.params[<span class="string">&#x27;W1&#x27;</span>], self.params[<span class="string">&#x27;W2&#x27;</span>]</span><br><span class="line">        b1, b2 = self.params[<span class="string">&#x27;b1&#x27;</span>], self.params[<span class="string">&#x27;b2&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">        a1 = np.dot(x, W1) + b1</span><br><span class="line">        z1 = sigmoid(a1)</span><br><span class="line">        a2 = np.dot(z1, W2) + b2</span><br><span class="line">        y = softmax(a2)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># x:输入数据, t:监督数据</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">self, x, t</span>):</span><br><span class="line">        y = self.predict(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> cross_entropy_error(y, t)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">self, x, t</span>):</span><br><span class="line">        y = self.predict(x)</span><br><span class="line">        y = np.argmax(y, axis=<span class="number">1</span>)</span><br><span class="line">        t = np.argmax(t, axis=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        accuracy = np.<span class="built_in">sum</span>(y == t) / <span class="built_in">float</span>(x.shape[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">return</span> accuracy</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># x:输入数据, t:监督数据</span></span><br><span class="line">    <span class="comment"># numerical_gradient(self, x, t)基于数值微分计算参数的梯度</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">numerical_gradient</span>(<span class="params">self, x, t</span>):</span><br><span class="line">        loss_W = <span class="keyword">lambda</span> W: self.loss(x, t)</span><br><span class="line">        </span><br><span class="line">        grads = &#123;&#125;</span><br><span class="line">        grads[<span class="string">&#x27;W1&#x27;</span>] = numerical_gradient(loss_W, self.params[<span class="string">&#x27;W1&#x27;</span>])</span><br><span class="line">        grads[<span class="string">&#x27;b1&#x27;</span>] = numerical_gradient(loss_W, self.params[<span class="string">&#x27;b1&#x27;</span>])</span><br><span class="line">        grads[<span class="string">&#x27;W2&#x27;</span>] = numerical_gradient(loss_W, self.params[<span class="string">&#x27;W2&#x27;</span>])</span><br><span class="line">        grads[<span class="string">&#x27;b2&#x27;</span>] = numerical_gradient(loss_W, self.params[<span class="string">&#x27;b2&#x27;</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> grads</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 使用误差反向传播法计算梯度，这里可先忽略</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">self, x, t</span>):</span><br><span class="line">        W1, W2 = self.params[<span class="string">&#x27;W1&#x27;</span>], self.params[<span class="string">&#x27;W2&#x27;</span>]</span><br><span class="line">        b1, b2 = self.params[<span class="string">&#x27;b1&#x27;</span>], self.params[<span class="string">&#x27;b2&#x27;</span>]</span><br><span class="line">        grads = &#123;&#125;</span><br><span class="line">        </span><br><span class="line">        batch_num = x.shape[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># forward</span></span><br><span class="line">        a1 = np.dot(x, W1) + b1</span><br><span class="line">        z1 = sigmoid(a1)</span><br><span class="line">        a2 = np.dot(z1, W2) + b2</span><br><span class="line">        y = softmax(a2)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># backward</span></span><br><span class="line">        dy = (y - t) / batch_num</span><br><span class="line">        grads[<span class="string">&#x27;W2&#x27;</span>] = np.dot(z1.T, dy)</span><br><span class="line">        grads[<span class="string">&#x27;b2&#x27;</span>] = np.<span class="built_in">sum</span>(dy, axis=<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        da1 = np.dot(dy, W2.T)</span><br><span class="line">        dz1 = sigmoid_grad(a1) * da1</span><br><span class="line">        grads[<span class="string">&#x27;W1&#x27;</span>] = np.dot(x.T, dz1)</span><br><span class="line">        grads[<span class="string">&#x27;b1&#x27;</span>] = np.<span class="built_in">sum</span>(dz1, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="mini-batch的实现"><a href="#mini-batch的实现" class="headerlink" title="mini-batch的实现"></a>mini-batch的实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> dataset.mnist <span class="keyword">import</span> load_mnist</span><br><span class="line"><span class="keyword">from</span> two_layer_net <span class="keyword">import</span> TwoLayerNet</span><br><span class="line"></span><br><span class="line">(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class="literal">True</span>, one_hot_</span><br><span class="line">laobel = <span class="literal">True</span>)</span><br><span class="line">train_loss_list = []</span><br><span class="line"><span class="comment"># 超参数</span></span><br><span class="line">iters_num = <span class="number">10000</span></span><br><span class="line">train_size = x_train.shape[<span class="number">0</span>]</span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">learning_rate = <span class="number">0.1</span></span><br><span class="line">network = TwoLayerNet(input_size=<span class="number">784</span>, hidden_size=<span class="number">50</span>, output_size=<span class="number">10</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iters_num):</span><br><span class="line">    <span class="comment"># 获取mini-batch</span></span><br><span class="line">    batch_mask = np.random.choice(train_size, batch_size)</span><br><span class="line">    x_batch = x_train[batch_mask]</span><br><span class="line">    t_batch = t_train[batch_mask]</span><br><span class="line">    <span class="comment"># 计算梯度</span></span><br><span class="line">    grad = network.numerical_gradient(x_batch, t_batch)</span><br><span class="line">    <span class="comment"># grad = network.gradient(x_batch, t_batch) # 高速版!</span></span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> (<span class="string">&#x27;W1&#x27;</span>, <span class="string">&#x27;b1&#x27;</span>, <span class="string">&#x27;W2&#x27;</span>, <span class="string">&#x27;b2&#x27;</span>):</span><br><span class="line">        network.params[key] -= learning_rate * grad[key]</span><br><span class="line">    <span class="comment"># 记录学习过程</span></span><br><span class="line">    loss = network.loss(x_batch, t_batch)</span><br><span class="line">    train_loss_list.append(loss)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="https://img.picgo.net/2024/09/11/sdxx344042649387b6a04c.png" alt="sdxx34"></p>
<p>可以发现随着学习的进行，损失函数的值在不断减小（即准确度逐渐升高)</p>
<p>这是学习正常进行的信号，表示神经网络的权重参数在逐渐拟合数据</p>
<p>不过这个损失函数的值，严格地讲是“对训练数据的某个mini-batch的损失函数”的值</p>
<p>训练数据的损失函数值减小</p>
<p>虽说是神经网络的学习正常进行的一个信号</p>
<p>但光看这个结果还不能说明该神经网络在其他数据集上也一定能有同等程度的表现</p>
<hr>
<p>神经网络学习的最初目标是掌握泛化能力</p>
<p>因此，要评价神经网络的泛化能力</p>
<p>就必须使用不包含在训练数据中的数据</p>
<p>下面的代码在进行学习的过程中</p>
<p>会定期地对训练数据和测试数据记录识别精度</p>
<p>这里，每经过一个epoch，都会记录下训练数据和测试数据的识别精度</p>
<blockquote>
<p><strong>epoch</strong>是一个单位</p>
<p>一个 epoch表示学习中所有训练数据均被使用过一次时的更新次数</p>
<p>比如，对于 10000笔训练数据</p>
<p>用大小为 100笔数据的mini-batch进行学习时</p>
<p>重复随机梯度下降法 100次，</p>
<p>所有的训练数据就都被“看过”了</p>
<p>此时，100次就是一个 epoch</p>
<p>(实际上</p>
<p>一般做法是事先将所有训练数据随机打乱</p>
<p>然后按指定的批次大小按序生成mini-batch</p>
<p>这样每个mini-batch均有一索引号</p>
<p>比如此例可以是0, 1, 2, … , 99，</p>
<p>然后用索引号可以遍历所有的mini-batch</p>
<p>遍历一次所有数据，就称为一个epoch)</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"><span class="keyword">import</span> sys, os</span><br><span class="line"></span><br><span class="line">sys.path.append(os.pardir)  <span class="comment"># 为了导入父目录的文件而进行的设定</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> dataset.mnist <span class="keyword">import</span> load_mnist</span><br><span class="line"><span class="keyword">from</span> two_layer_net <span class="keyword">import</span> TwoLayerNet</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读入数据</span></span><br><span class="line">(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class="literal">True</span>, one_hot_label=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">network = TwoLayerNet(input_size=<span class="number">784</span>, hidden_size=<span class="number">50</span>, output_size=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参数</span></span><br><span class="line">iters_num = <span class="number">10000</span>  <span class="comment"># 适当设定循环的次数</span></span><br><span class="line">train_size = x_train.shape[<span class="number">0</span>]</span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">learning_rate = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 记录训练过程</span></span><br><span class="line">train_loss_list = []</span><br><span class="line">train_acc_list = []</span><br><span class="line">test_acc_list = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 平均每个epoch的重复次数</span></span><br><span class="line">iter_per_epoch = <span class="built_in">max</span>(train_size / batch_size, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iters_num):</span><br><span class="line">    <span class="comment"># 随机选取batch数据</span></span><br><span class="line">    batch_mask = np.random.choice(train_size, batch_size)</span><br><span class="line">    x_batch = x_train[batch_mask]</span><br><span class="line">    t_batch = t_train[batch_mask]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算梯度</span></span><br><span class="line">    <span class="comment"># grad = network.numerical_gradient(x_batch, t_batch)</span></span><br><span class="line">    grad = network.gradient(x_batch, t_batch)   <span class="comment"># 使用反向传播求导加速</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> (<span class="string">&#x27;W1&#x27;</span>, <span class="string">&#x27;b1&#x27;</span>, <span class="string">&#x27;W2&#x27;</span>, <span class="string">&#x27;b2&#x27;</span>):</span><br><span class="line">        network.params[key] -= learning_rate * grad[key]</span><br><span class="line"></span><br><span class="line">    loss = network.loss(x_batch, t_batch)</span><br><span class="line">    train_loss_list.append(loss)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> i % iter_per_epoch == <span class="number">0</span>:</span><br><span class="line">        train_acc = network.accuracy(x_train, t_train)</span><br><span class="line">        test_acc = network.accuracy(x_test, t_test)</span><br><span class="line">        train_acc_list.append(train_acc)</span><br><span class="line">        test_acc_list.append(test_acc)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;train acc, test acc | &quot;</span> + <span class="built_in">str</span>(train_acc) + <span class="string">&quot;, &quot;</span> + <span class="built_in">str</span>(test_acc))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制图形</span></span><br><span class="line">markers = &#123;<span class="string">&#x27;train&#x27;</span>: <span class="string">&#x27;o&#x27;</span>, <span class="string">&#x27;test&#x27;</span>: <span class="string">&#x27;s&#x27;</span>&#125;</span><br><span class="line">x = np.arange(<span class="built_in">len</span>(train_acc_list))</span><br><span class="line">plt.plot(x, train_acc_list, label=<span class="string">&#x27;train acc&#x27;</span>)</span><br><span class="line">plt.plot(x, test_acc_list, label=<span class="string">&#x27;test acc&#x27;</span>, linestyle=<span class="string">&#x27;--&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;epochs&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;accuracy&quot;</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">1.0</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;lower right&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>每经过一个epoch，就对所有的训练数据和测试数据计算识别精度，并记录结果</p>
<p>之所以要计算每一个epoch的识别精度</p>
<p>是因为如果在for语句的循环中一直计算识别精度</p>
<p>会花费太多时间</p>
<p>并且，也没有必要那么频繁地记录识别精度</p>
<p>（只要从大方向上大致把握识别精度的推移就可以了）</p>
<p>因此，我们才会每经过一个epoch就记录一次训练数据的识别精度</p>
<p>运行结果图示：<img src="https://img.picgo.net/2024/09/11/sdxx3526de6dabc48b71bb.png" alt="sdxx35"></p>
<p>实线表示训练数据的识别精度</p>
<p>虚线表示测试数据的识别精度</p>
<p>这两个识别精度基本上没有差异（两条线基本重叠在一起）</p>
<p>因此，可以说这次的学习中没有发生过拟合的现象</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://XTERNALXLUE.github.io">XTERNALXLUE</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://xternalxlue.github.io/2024/09/10/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E3%80%91%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">https://xternalxlue.github.io/2024/09/10/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E3%80%91%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://XTERNALXLUE.github.io" target="_blank">XTERNALXLUE's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a></div><div class="post_share"><div class="social-share" data-image="/./img/bg6.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/09/11/%E3%80%90%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E3%80%91%E6%B4%9B%E8%B0%B7-P1443-%E9%A9%AC%E7%9A%84%E9%81%8D%E5%8E%86/" title="【每日一题】洛谷-P1443-马的遍历"><img class="cover" src="/./img/bg50.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">【每日一题】洛谷-P1443-马的遍历</div></div></a></div><div class="next-post pull-right"><a href="/2024/09/10/%E3%80%90%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E3%80%91%E6%B4%9B%E8%B0%B7-P1162-%E5%A1%AB%E6%B6%82%E9%A2%9C%E8%89%B2/" title="【每日一题】洛谷-P1162-填涂颜色"><img class="cover" src="/./img/bg40.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">【每日一题】洛谷-P1162-填涂颜色</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/10/29/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E3%80%91%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="【深度学习入门】卷积神经网络"><img class="cover" src="/./img/bg32.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-10-29</div><div class="title">【深度学习入门】卷积神经网络</div></div></a></div><div><a href="/2024/10/09/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E3%80%91%E5%8F%82%E6%95%B0%E7%9A%84%E6%9B%B4%E6%96%B0/" title="【深度学习入门】参数的更新"><img class="cover" src="/./img/bg6.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-10-09</div><div class="title">【深度学习入门】参数的更新</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/./img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">XTERNALXLUE</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">405</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">75</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/XTERNALXLUE"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">hh</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E3%80%91%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.</span> <span class="toc-text">【深度学习入门】神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#sigmoid%E5%87%BD%E6%95%B0"><span class="toc-number">1.1.</span> <span class="toc-text">sigmoid函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ReLU%E5%87%BD%E6%95%B0"><span class="toc-number">1.2.</span> <span class="toc-text">ReLU函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%86%85%E7%A7%AF"><span class="toc-number">1.3.</span> <span class="toc-text">神经网络内积</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.4.</span> <span class="toc-text">3层神经网络的实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#softmax%E5%87%BD%E6%95%B0"><span class="toc-number">1.5.</span> <span class="toc-text">softmax函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E6%98%AFMNIST%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E9%9B%86"><span class="toc-number">1.6.</span> <span class="toc-text">使用的数据集是MNIST手写数字图像集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE"><span class="toc-number">1.7.</span> <span class="toc-text">均方误差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#one-hot%E8%A1%A8%E7%A4%BA"><span class="toc-number">1.8.</span> <span class="toc-text">one-hot表示</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E8%AF%AF%E5%B7%AE"><span class="toc-number">1.9.</span> <span class="toc-text">交叉熵误差</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%89%8D%E6%8F%90"><span class="toc-number">1.9.1.</span> <span class="toc-text">前提</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%B1%BB"><span class="toc-number">1.10.</span> <span class="toc-text">2层神经网络的类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mini-batch%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.11.</span> <span class="toc-text">mini-batch的实现</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/06/26/%E3%80%90%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E3%80%91%E6%B4%9B%E8%B0%B7-P1125-NOIP2008%E6%8F%90%E9%AB%98%E7%BB%84-%E7%AC%A8%E5%B0%8F%E7%8C%B4/" title="【每日一题】洛谷-P1125-[NOIP 2008 提高组] 笨小猴"><img src="/./img/bg31.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【每日一题】洛谷-P1125-[NOIP 2008 提高组] 笨小猴"/></a><div class="content"><a class="title" href="/2025/06/26/%E3%80%90%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E3%80%91%E6%B4%9B%E8%B0%B7-P1125-NOIP2008%E6%8F%90%E9%AB%98%E7%BB%84-%E7%AC%A8%E5%B0%8F%E7%8C%B4/" title="【每日一题】洛谷-P1125-[NOIP 2008 提高组] 笨小猴">【每日一题】洛谷-P1125-[NOIP 2008 提高组] 笨小猴</a><time datetime="2025-06-26T03:24:44.000Z" title="发表于 2025-06-26 11:24:44">2025-06-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/06/25/%E3%80%90%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E3%80%91%E6%B4%9B%E8%B0%B7-B2106-%E7%9F%A9%E9%98%B5%E8%BD%AC%E7%BD%AE/" title="【每日一题】洛谷-B2106-矩阵转置"><img src="/./img/bg16.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【每日一题】洛谷-B2106-矩阵转置"/></a><div class="content"><a class="title" href="/2025/06/25/%E3%80%90%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E3%80%91%E6%B4%9B%E8%B0%B7-B2106-%E7%9F%A9%E9%98%B5%E8%BD%AC%E7%BD%AE/" title="【每日一题】洛谷-B2106-矩阵转置">【每日一题】洛谷-B2106-矩阵转置</a><time datetime="2025-06-25T02:11:55.000Z" title="发表于 2025-06-25 10:11:55">2025-06-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/06/24/%E3%80%90%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E3%80%91%E6%B4%9B%E8%B0%B7-B2160-%E7%97%85%E4%BA%BA%E6%8E%92%E9%98%9F/" title="【每日一题】洛谷-B2160-病人排队"><img src="/./img/bg22.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【每日一题】洛谷-B2160-病人排队"/></a><div class="content"><a class="title" href="/2025/06/24/%E3%80%90%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E3%80%91%E6%B4%9B%E8%B0%B7-B2160-%E7%97%85%E4%BA%BA%E6%8E%92%E9%98%9F/" title="【每日一题】洛谷-B2160-病人排队">【每日一题】洛谷-B2160-病人排队</a><time datetime="2025-06-24T05:34:11.000Z" title="发表于 2025-06-24 13:34:11">2025-06-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/06/23/%E3%80%90%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E3%80%91%E6%B4%9B%E8%B0%B7-B2125-%E6%9C%80%E9%AB%98%E5%88%86%E6%95%B0%E7%9A%84%E5%AD%A6%E7%94%9F%E5%A7%93%E5%90%8D/" title="【每日一题】洛谷-B2125-最高分数的学生姓名"><img src="/./img/bg6.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【每日一题】洛谷-B2125-最高分数的学生姓名"/></a><div class="content"><a class="title" href="/2025/06/23/%E3%80%90%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E3%80%91%E6%B4%9B%E8%B0%B7-B2125-%E6%9C%80%E9%AB%98%E5%88%86%E6%95%B0%E7%9A%84%E5%AD%A6%E7%94%9F%E5%A7%93%E5%90%8D/" title="【每日一题】洛谷-B2125-最高分数的学生姓名">【每日一题】洛谷-B2125-最高分数的学生姓名</a><time datetime="2025-06-23T07:16:48.000Z" title="发表于 2025-06-23 15:16:48">2025-06-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/06/22/%E3%80%90%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E3%80%91%E6%B4%9B%E8%B0%B7-B2115-%E5%AF%86%E7%A0%81%E7%BF%BB%E8%AF%91/" title="【每日一题】洛谷-B2115-密码翻译"><img src="/./img/bg1.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【每日一题】洛谷-B2115-密码翻译"/></a><div class="content"><a class="title" href="/2025/06/22/%E3%80%90%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E3%80%91%E6%B4%9B%E8%B0%B7-B2115-%E5%AF%86%E7%A0%81%E7%BF%BB%E8%AF%91/" title="【每日一题】洛谷-B2115-密码翻译">【每日一题】洛谷-B2115-密码翻译</a><time datetime="2025-06-22T15:39:06.000Z" title="发表于 2025-06-22 23:39:06">2025-06-22</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2024 - 2025 By XTERNALXLUE</div><div class="footer_custom_text">愿世间再无痛苦</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.14.0-b3"></script><script src="/js/main.js?v=4.14.0-b3"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.35/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>(() => {
  const runMermaid = (ele) => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    Array.from(ele).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return
    
    codeMermaidEle.forEach(ele => {
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.innerHTML = `<pre class="mermaid-src" hidden>${ele.textContent}</pre>`
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (false) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid@10.9.0/dist/mermaid.min.js').then(runMermaidFn)
  }
  
  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.14.0-b3"></script></div></div></body></html>